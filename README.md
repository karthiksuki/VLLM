# VLLM

This project implements a Visual Language Model (VLM)-powered multi-modal search engine using OpenAI’s CLIP and the COCO dataset. It enables users to retrieve semantically relevant images based on either a text prompt (e.g., “A Panda”) or an example image. The system uses CLIP to embed both images and text into a shared latent space and leverages FAISS for efficient similarity-based indexing and retrieval. Results are presented through an intuitive Gradio web interface, offering an interactive and scalable solution for content-based image search.


